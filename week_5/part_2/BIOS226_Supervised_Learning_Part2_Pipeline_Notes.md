# BIOS226 -- Supervised Learning (Part 2)

## The Tumor Subtype Classification Pipeline

------------------------------------------------------------------------

# From Data to Decision: The Pipeline

We will use our synthetic breast cancer gene expression dataset
generated in R.

-   120 patients\
-   10,000 gene features\
-   Subtypes: Luminal_A vs Basal_like\
-   Logistic regression classifier

Goal: Predict tumor subtype from gene expression profiles.

------------------------------------------------------------------------

# 1. Define the Prediction Problem

Clinical question: Can we classify a tumor as Basal_like or Luminal_A
based on gene expression?

Model task: Binary classification.

-   Input: 10,000 gene expression values per patient\
-   Output: Probability tumor is Basal_like

Positive class: Basal_like

------------------------------------------------------------------------

# 2. Validate the Data Structure

Before modeling, we verify:

-   Column structure (Patient_ID, Subtype, Gene_1 ... Gene_n)
-   Exactly two classes present
-   All gene columns are numeric
-   No schema inconsistencies

Why? Garbage in leads to garbage out.

------------------------------------------------------------------------

# 3. Stratified Train/Test Split

We split the data:

-   80 percent Training
-   20 percent Test
-   Stratified by subtype

Why stratified? Maintains class balance in both sets.

The test set is held out and never used during training.

------------------------------------------------------------------------

# 4. Feature Selection (Train Only)

We rank genes by: Absolute difference in mean expression between
classes.

Select top K genes (e.g., 25).

Important: Feature selection is performed on training data only.

Why? Avoid data leakage.

------------------------------------------------------------------------

# 5. Scaling & Normalisation (Train Only)

Gene expression values vary in magnitude.

We standardize each selected gene:

-   Subtract mean
-   Divide by standard deviation

Scaling parameters are learned from training data only and applied to
test data.

------------------------------------------------------------------------

# 6. Cross-Validation on Training Data

We perform k-fold cross-validation (e.g., 10-fold):

-   Split training set into 10 parts
-   Train on 9 folds
-   Validate on 1 fold
-   Repeat 10 times

Outputs: - AUC per fold - Precision per fold

Purpose: Estimate model stability before touching test set.

------------------------------------------------------------------------

# 7. Fit the Logistic Regression Model

Logistic regression estimates:

P(Basal_like \| gene expression)

Model form:

log(p / (1 − p)) = β0 + β1x1 + β2x2 + ...

Output: Probability between 0 and 1 for each patient.

------------------------------------------------------------------------

# 8. Confusion Matrix: Understanding Errors

Using chosen threshold (e.g., 0.6):

We compute:

-   True Positives (TP)
-   False Positives (FP)
-   True Negatives (TN)
-   False Negatives (FN)

From this we calculate:

-   Precision
-   Sensitivity
-   Specificity

This describes performance at one threshold.

------------------------------------------------------------------------

## INSERT CONFUSION MATRIX FIGURE HERE

Example file generated by script: `truth_table_test_seed_XXXX.png`

(Replace XXXX with your model seed.)

------------------------------------------------------------------------

# 9. ROC Curve & AUC

The model outputs probabilities.

If we vary the threshold: Sensitivity and False Positive Rate change.

ROC curve shows performance across all thresholds.

-   X-axis: False Positive Rate
-   Y-axis: True Positive Rate

AUC measures discrimination ability: 0.5 means random 1.0 means perfect

------------------------------------------------------------------------

## INSERT ROC CURVE FIGURE HERE

Example file generated by script: `roc_curve_test_seed_XXXX.png`

This figure includes: - Cross-validation mean ROC - CV envelope (mean
+/- 1 SD) - Final test ROC curve

------------------------------------------------------------------------

# 10. Final Test Evaluation and Discipline

After all training steps:

We evaluate once on the held-out test set.

We report:

-   Confusion matrix
-   Precision
-   ROC AUC
-   MSE (probability-based)
-   R squared (probability-based)

This gives an unbiased estimate of real-world performance.

------------------------------------------------------------------------

# Pipeline Summary

1.  Define question\
2.  Validate data\
3.  Split train/test\
4.  Select features (train only)\
5.  Scale data (train only)\
6.  Cross-validate\
7.  Fit model\
8.  Evaluate with confusion matrix\
9.  Evaluate with ROC/AUC\
10. Report final test performance

Everything in this workflow protects us from overfitting and leakage.
